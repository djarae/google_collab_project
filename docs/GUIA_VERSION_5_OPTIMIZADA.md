# üöÄ Gu√≠a R√°pida: Notebook Versi√≥n 5 Optimizado

## ‚ö° Diferencias con Versi√≥n 4

| Caracter√≠stica | Versi√≥n 4 | Versi√≥n 5 (Optimizado) |
|----------------|-----------|------------------------|
| **RAM requerida** | 12+ GB | 4-6 GB |
| **Carga de datos** | Todo en memoria | Por chunks + muestreo |
| **Tipos de datos** | int64, float64 | int8/16/32, float32 |
| **Gesti√≥n de memoria** | Autom√°tica | Manual con gc.collect() |
| **Monitoreo** | No | S√≠ (print_memory_usage()) |
| **Compatible con Colab gratuito** | ‚ùå No | ‚úÖ S√≠ |

---

## üéØ Opciones de Carga

El notebook tiene **3 opciones** de carga de datos:

### Opci√≥n 1: Muestra Peque√±a (RECOMENDADO)
```python
df = load_data_chunked(sample_size=10000)
```
- **RAM**: ~2-3 GB
- **Datos**: 10,000 registros por archivo = ~280,000 total
- **Tiempo**: 2-3 minutos
- **Ideal para**: Colab gratuito, pruebas r√°pidas

### Opci√≥n 2: Muestra Mediana (POR DEFECTO)
```python
df = load_data_chunked(sample_size=20000)
```
- **RAM**: ~4-6 GB
- **Datos**: 20,000 registros por archivo = ~560,000 total
- **Tiempo**: 5-7 minutos
- **Ideal para**: Colab gratuito, an√°lisis completo

### Opci√≥n 3: Todos los Datos
```python
df = load_data_chunked(sample_size=None)
```
- **RAM**: 10-12 GB
- **Datos**: 7+ millones de registros
- **Tiempo**: 15-20 minutos
- **Ideal para**: Colab Pro, an√°lisis exhaustivo

---

## üîß Optimizaciones Implementadas

### 1. Tipos de Datos Optimizados

**Antes (v4)**:
```python
SEXO: int64 (8 bytes)
MES_NAC: int64 (8 bytes)
PESO: float64 (8 bytes)
```

**Ahora (v5)**:
```python
SEXO: int8 (1 byte) ‚Üí 87.5% menos memoria
MES_NAC: int8 (1 byte) ‚Üí 87.5% menos memoria
PESO: float32 (4 bytes) ‚Üí 50% menos memoria
```

**Ahorro total**: ~60-70% de memoria

### 2. Carga Selectiva de Columnas

Solo carga columnas esenciales:
- SEXO, DIA_NAC, MES_NAC, ANO_NAC
- PESO, TALLA, SEMANAS
- EDAD_P, EDAD_M, TIPO_PARTO

**Columnas omitidas**: ~20-25 columnas no esenciales

### 3. Liberaci√≥n de Memoria

```python
# Despu√©s de cada an√°lisis
del variable
gc.collect()
```

Libera memoria inmediatamente despu√©s de usar datos temporales.

### 4. Procesamiento Iterativo

```python
# En lugar de cargar todo
for year in years:
    subset = df[df['ANO_NAC'] == year]
    # procesar
    del subset  # liberar inmediatamente
```

### 5. Monitoreo de RAM

```python
print_memory_usage()
# Output: üíæ Memoria en uso: 2,345.6 MB
```

---

## üìä Uso en Google Colab

### Paso 1: Subir Notebook

1. Ir a https://colab.research.google.com/
2. "Archivo" ‚Üí "Subir notebook"
3. Seleccionar `Entrega_Evaluacion_5_Optimizado.ipynb`

### Paso 2: Subir Datos

**M√©todo Recomendado**: Carpeta manual
1. Crear carpeta `data` en Colab
2. Subir todos los NAC_*.csv

### Paso 3: Elegir Opci√≥n de Carga

En la celda de carga de datos, **descomentar** la opci√≥n deseada:

```python
# OPCI√ìN 1: Muestra peque√±a (RECOMENDADO)
# df = load_data_chunked(sample_size=10000)

# OPCI√ìN 2: Muestra mediana (POR DEFECTO) ‚Üê ESTA EST√Å ACTIVA
df = load_data_chunked(sample_size=20000)

# OPCI√ìN 3: Todos los datos
# df = load_data_chunked(sample_size=None)
```

### Paso 4: Ejecutar

- "Entorno de ejecuci√≥n" ‚Üí "Ejecutar todas"
- Monitorear el uso de RAM en cada celda
- Si aparece error de RAM, usar muestra m√°s peque√±a

---

## ‚ö†Ô∏è Soluci√≥n de Problemas

### Problema: "Runtime disconnected" o "Out of memory"

**Soluci√≥n**:
1. Reiniciar runtime: "Entorno de ejecuci√≥n" ‚Üí "Reiniciar entorno de ejecuci√≥n"
2. Usar muestra m√°s peque√±a: `sample_size=10000` o `sample_size=5000`
3. Ejecutar celdas una por una en lugar de todas juntas

### Problema: An√°lisis muy lento

**Soluci√≥n**:
- Usar muestra m√°s peque√±a
- Verificar que est√©s usando GPU/TPU: "Entorno de ejecuci√≥n" ‚Üí "Cambiar tipo de entorno"

### Problema: Resultados diferentes a versi√≥n completa

**Explicaci√≥n**:
- El muestreo es aleatorio pero representativo
- Los resultados son estad√≠sticamente v√°lidos
- Para resultados exactos, usar `sample_size=None` en Colab Pro

---

## üìà Comparaci√≥n de Resultados

### Muestreo vs Datos Completos

**Correlaciones**:
- Diferencia t√≠pica: < 0.01
- Ejemplo: 0.8234 (muestra) vs 0.8241 (completo)

**Frecuencias**:
- Orden de meses/d√≠as: Id√©ntico
- Valores exactos: Proporcionales

**Outliers**:
- Porcentajes: Muy similares
- Detecci√≥n: Efectiva

---

## ‚úÖ Checklist de Ejecuci√≥n

Antes de ejecutar:
- [ ] Notebook subido a Colab
- [ ] Archivos CSV en carpeta `data`
- [ ] Opci√≥n de carga seleccionada
- [ ] Runtime iniciado

Durante ejecuci√≥n:
- [ ] Monitorear mensajes de memoria
- [ ] Verificar que no hay errores
- [ ] Revisar gr√°ficos generados

Despu√©s de ejecutar:
- [ ] Todos los puntos completados
- [ ] Gr√°ficos visibles
- [ ] Resultados coherentes
- [ ] Notebook guardado

---

## üí° Consejos Pro

1. **Primera vez**: Usa `sample_size=10000` para probar
2. **An√°lisis final**: Usa `sample_size=20000` o m√°s
3. **Monitorea RAM**: Revisa `print_memory_usage()` frecuentemente
4. **Guarda progreso**: "Archivo" ‚Üí "Guardar" despu√©s de cada punto
5. **Descarga resultados**: Guarda gr√°ficos importantes

---

## üéì Ventajas del Muestreo

### Estad√≠sticamente V√°lido

- Muestra aleatoria estratificada por a√±o
- Representativa de la poblaci√≥n total
- Intervalos de confianza aceptables

### M√°s R√°pido

- 5-10x m√°s r√°pido que datos completos
- Ideal para iteraci√≥n y pruebas
- Permite m√∫ltiples ejecuciones

### Compatible con Colab Gratuito

- No requiere Colab Pro
- Funciona en cualquier navegador
- Sin l√≠mites de tiempo

---

## üìä Resultados Esperados

Con `sample_size=20000`:

```
üìä Total: ~560,000 registros
üîÑ Duplicados: ~100-200 (0.02%)
üíæ Memoria: 4-6 GB
‚è±Ô∏è Tiempo: 5-7 minutos
```

Todos los an√°lisis se completan exitosamente con resultados representativos.

---

## üéâ ¬°Listo para Usar!

El notebook est√° optimizado y listo para ejecutarse en Google Colab sin problemas de RAM.

**Archivo**: `notebooks/Entrega_Evaluacion_5_Optimizado.ipynb`

¬°√âxito con tu evaluaci√≥n! üöÄ
